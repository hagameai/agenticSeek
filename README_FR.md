


# AgenticSeek: Une IA comme Manus mais √† base d'agents DeepSeek R1 fonctionnant en local.

Une alternative **enti√®rement locale** √† Manus AI, un assistant vocal IA qui code, explore votre syst√®me de fichiers, navigue sur le web et corrige ses erreurs, tout cela sans envoyer la moindre donn√©e dans le cloud. Construit avec des mod√®les de raisonnement comme DeepSeek R1, cet agent autonome fonctionne enti√®rement sur votre hardware, garantissant la confidentialit√© de vos donn√©es.

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/4Ub2D6Fj)

> üõ†Ô∏è **En cours de d√©veloppement** ‚Äì On cherche activement des contributeurs!

![alt text](./media/whale_readme.jpg)

> *Do a deep search of AI startup in Osaka and Tokyo, find at least 5, then save in the research_japan.txt file*

> *Can you make a tetris game in C ?*

> *I would like to setup a new project file index as mark2.*


### agenticSeek peut planifier des taches!

![alt text](./media/exemples/demo_image.png)

## Fonctionnalit√©s:

- **100% Local**: Fonctionne en local sur votre PC. Vos donn√©es restent les v√¥tres. 

- **Acc√®s √† vos Fichiers**: Utilise bash pour naviguer et manipuler vos fichiers.

- **Codage semi-autonome**: Peut √©crire, d√©boguer et ex√©cuter du code en Python, C, Golang et d'autres langages √† venir. 

- **Routage d'Agent**: S√©lectionne automatiquement l‚Äôagent appropri√© pour la t√¢che. 

- **Planification**: Pour les taches complexe utilise plusieurs agents.

- **Navigation Web Autonome**: Navigation web autonome.

- **Memoire efficace**: Gestion efficace de la m√©moire et des sessions. 

---

## **Installation**

Assurez-vous d‚Äôavoir install√© le pilote Chrome, Docker et Python 3.10 (ou une version plus r√©cente).

Pour les probl√®mes li√©s au pilote Chrome, consultez la section Chromedriver.

### 1Ô∏è‚É£ Cloner le d√©p√¥t et configurer

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2 **Cr√©er un environnement virtuel**

```sh
python3 -m venv agentic_seek_env
source agentic_seek_env/bin/activate     
# On Windows: agentic_seek_env\Scripts\activate
```

### 3Ô∏è‚É£ **Installation**

**Automatique:**

```sh
./install.sh
```

**Manuel:**

```sh
pip3 install -r requirements.txt
```


## Faire fonctionner sur votre machine 

**Nous recommandons d‚Äôutiliser au moins DeepSeek 14B, les mod√®les plus petits ont du mal avec l‚Äôutilisation des outils et oublient rapidement le contexte.**

### 1Ô∏è‚É£ **T√©l√©chargement du mod√®le**  

Assurer vous d'avoir [Ollama](https://ollama.com/) install√©.

T√©l√©charger `deepseek-r1:14b` de [DeepSeek](https://deepseek.com/models)

```sh
ollama pull deepseek-r1:14b
```

### 2Ô∏è **D√©marrage d'ollama**  

```sh
ollama serve
```

Modifiez le fichier config.ini pour d√©finir provider_name sur ollama et provider_model sur deepseek-r1:14b

```sh
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:14b
provider_server_address = 127.0.0.1:11434
```

d√©marrer tous les services :

```sh
sudo ./start_services.sh
```

Lancer l'assitant:

```sh
python3 main.py
```

Voir la section **Utilisation** si vous ne comprenez pas comment l‚Äôutiliser

Voir la section **Probl√®mes** connus si vous rencontrez des probl√®mes

Voir la section **Ex√©cuter** avec une API si votre mat√©riel ne peut pas ex√©cuter DeepSeek localement

Voir la section **Configuration** pour une explication d√©taill√©e du fichier de configuration.

---

## Utilisation

Avertissement : actuellement, le syst√®me qui choisit le meilleur agent IA fonctionnera mal avec du texte non anglophone. Cela est d√ª au fait que le routage des agents utilise un mod√®le entra√Æn√© sur du texte en anglais. Nous travaillons dur pour corriger cela. Veuillez utiliser l‚Äôanglais pour le moment.

Assurez-vous que les services sont en cours d‚Äôex√©cution avec ./start_services.sh et lancez AgenticSeek avec python3 main.py

```sh
sudo ./start_services.sh
python3 main.py
```

Vous verrez un prompt: ">>> "
Cela indique qu‚ÄôAgenticSeek attend que vous saisissiez des instructions.
Vous pouvez √©galement utiliser la reconnaissance vocale en d√©finissant listen = True dans la configuration.

Pour quitter, dites simplement `goodbye`.

Voici quelques exemples d‚Äôutilisation :

### Programmation

> *Help me with matrix multiplication in Golang*

> *Scan my network with nmap, find if any suspicious devices is connected*

> *Make a snake game in python*

### Recherche web

> *Do a web search to find cool tech startup in Japan working on cutting edge AI research*

> *Can you find on the internet who created agenticSeek?*

> *Can you find on which website I can buy a rtx 4090 for cheap*

### Fichier

> *Hey can you find where is million_dollars_contract.pdf i lost it*

> *Show me how much space I have left on my disk*

> *Find and read the README.md and follow the install instruction*

### Conversation

> *Tell me about France*

> *What is the meaning of life ?*

> *Should I take creatine before or after workout?*


Apr√®s avoir saisi votre requ√™te, AgenticSeek attribuera le meilleur agent pour la t√¢che.

Comme il s‚Äôagit d‚Äôun prototype, le syst√®me de routage des agents pourrait ne pas toujours attribuer le bon agent en fonction de votre requ√™te.

Par cons√©quent, vous devez √™tre explicite sur ce que vous voulez et sur la mani√®re dont l‚ÄôIA doit proc√©der. Par exemple, si vous voulez qu‚Äôelle effectue une recherche sur le web, ne dites pas :

Connait-tu de bons pays pour voyager seul ?

Dites plut√¥t :

Fait une recherche sur le web, quels sont les meilleurs pays pour voyager seul?

---

## **Ex√©cuter le LLM sur votre propre serveur**  

Si vous disposez d‚Äôun ordinateur puissant ou d‚Äôun serveur que vous voulez utiliser, mais que vous souhaitez y acc√©der depuis votre ordinateur portable, vous avez la possibilit√© d‚Äôex√©cuter le LLM sur un serveur distant.

### 1Ô∏è‚É£  **Configurer et d√©marrer les scripts du serveur** 

Sur votre "serveur" qui ex√©cutera le mod√®le IA, obtenez l‚Äôadresse IP

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1
```

Remarque : Pour Windows ou macOS, utilisez respectivement ipconfig ou ifconfig pour trouver l‚Äôadresse IP.

**Si vous souhaitez utiliser un fournisseur bas√© sur OpenAI, suivez la section Ex√©cuter avec une API.**

Clonez le d√©p√¥t et entrez dans le dossier server/.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/server/
```

Installez les d√©pendances sp√©cifiques au serveur :

```sh
pip3 install -r requirements.txt
```

Ex√©cutez le script du serveur.

```sh
python3 app.py --provider ollama --port 3333
```

Vous avez le choix entre utiliser ollama et llamacpp comme service LLM.

### 2Ô∏è‚É£ **Lancer** 

Maintenant, sur votre ordinateur personnel :

Modifiez le fichier config.ini pour d√©finir provider_name sur server et provider_model sur deepseek-r1:14b.

D√©finissez provider_server_address sur l‚Äôadresse IP de la machine qui ex√©cutera le mod√®le.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:14b
provider_server_address = x.x.x.x:3333
```

Ex√©cutez l‚Äôassistant :

```sh
sudo ./start_services.sh
python3 main.py
```

## **Ex√©cuter avec une API**  

AVERTISSEMENT : Assurez-vous qu‚Äôil n‚Äôy a pas d‚Äôespace en fin de ligne dans la configuration.

D√©finissez is_local sur True si vous utilisez une API bas√©e sur OpenAI localement.

Changez l‚Äôadresse IP si votre API bas√©e sur OpenAI fonctionne sur votre propre serveur.

```sh
[MAIN]
is_local = False
provider_name = openai
provider_model = gpt-4o
provider_server_address = 127.0.0.1:5000
```

Ex√©cutez l‚Äôassistant :

```sh
sudo ./start_services.sh
python3 main.py
```

## Config

Exemple de configuration :
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:1.5b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
work_dir =  /Users/mlg/Documents/ai_folder
jarvis_personality = False
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explanation**:

`is_local` -> Ex√©cute l‚Äôagent localement (True) ou sur un serveur distant (False).

`provider_name` -> Le fournisseur √† utiliser (parmi : ollama, server, lm-studio, deepseek-api).

`provider_model` -> Le mod√®le utilis√©, par exemple, deepseek-r1:1.5b.

`provider_server_address` -> Adresse du serveur, par exemple, 127.0.0.1:11434 pour local. D√©finissez n‚Äôimporte quoi pour une API non locale.

`agent_name` -> Nom de l‚Äôagent, par exemple, Friday. Utilis√© comme mot d√©clencheur pour la reconnaissance vocale.

`recover_last_session` -> Reprend la derni√®re session (True) ou non (False).

`save_session` -> Sauvegarde les donn√©es de la session (True) ou non (False).

`speak` -> Active la sortie vocale (True) ou non (False).

`listen` -> √âcoute les entr√©es vocales (True) ou non (False).

`work_dir` -> Dossier auquel l‚ÄôIA aura acc√®s, par exemple : /Users/user/Documents/.

`jarvis_personality` -> Utilise une personnalit√© de type JARVIS (True) ou non (False). Cela modifie simplement le fichier de prompt.

`headless_browser` -> Ex√©cute le navigateur sans fen√™tre visible (True) ou non (False).

`stealth_mode` -> Rend la d√©tection des bots plus difficile. Le seul inconv√©nient est que vous devez installer manuellement l‚Äôextension anticaptcha.



## Providers

Le tableau ci-dessous montre les fournisseurs disponibles :

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Ex√©cutez des LLM localement avec facilit√© en utilisant Ollama comme fournisseur LLM 
| server    | Yes    | H√©bergez le mod√®le sur une autre machine, ex√©cutez sur votre machine locale 
| lm-studio  | Yes    | Ex√©cutez un LLM localement avec LM Studio (d√©finissez provider_name sur lm-studio) 
| openai    | No     | Utilise ChatGPT API (pas priv√©) |
| deepseek-api  | No     | Deepseek API (pas priv√©) |
| huggingface| No    | Hugging-Face API (pas priv√©) |

Pour s√©lectionner un fournisseur, modifiez le config.ini :

```
is_local = False
provider_name = openai
provider_model = gpt-4o
provider_server_address = 127.0.0.1:5000
```

`is_local` : doit √™tre True pour tout LLM ex√©cut√© localement, sinon False.

`provider_name` : S√©lectionnez le fournisseur √† utiliser par son nom, voir la liste des fournisseurs ci-dessus.

`provider_model` : D√©finissez le mod√®le √† utiliser par l‚Äôagent.

`provider_server_address` : peut √™tre d√©fini sur n‚Äôimporte quoi si vous n‚Äôutilisez pas le fournisseur server.

# Probl√®mes connus 

## Probl√®mes avec Chromedriver

Erreur #1:**incompatibilit√©**

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Cela se produit s‚Äôil y a une incompatibilit√© entre votre navigateur et la version de chromedriver.

Vous devez naviguer pour t√©l√©charger la derni√®re version :

https://developer.chrome.com/docs/chromedriver/downloads

Si vous utilisez Chrome version 115 ou plus r√©cent, allez sur :

https://googlechromelabs.github.io/chrome-for-testing/

Et t√©l√©chargez la version de chromedriver correspondant √† votre syst√®me d‚Äôexploitation.

![alt text](./media/chromedriver_readme.png)

Si cette section est incompl√®te, veuillez signaler un probl√®me.

## FAQ

**Q: What hardware do I need?**  

Mod√®le 7B : GPU avec 8 Go de VRAM.
Mod√®le 14B : GPU 12 Go (par exemple, RTX 3060).
Mod√®le 32B : 24 Go+ de VRAM.

**Q: Why Deepseek R1 over other models?**  

DeepSeek R1 excelle dans le raisonnement et l‚Äôutilisation d‚Äôoutils pour sa taille. Nous pensons que c‚Äôest un choix solide pour nos besoins, bien que d‚Äôautres mod√®les fonctionnent √©galement bien, DeepSeek est notre choix principal.

**Q: I get an error running `main.py`. What do I do?**  

Assurez-vous qu‚ÄôOllama est en cours d‚Äôex√©cution (ollama serve), que votre config.ini correspond √† votre fournisseur, et que les d√©pendances sont install√©es. Si cela ne fonctionne pas, n‚Äôh√©sitez pas √† signaler un probl√®me.

**Q: Can it really run 100% locally?**  

Oui, avec les fournisseurs Ollama ou Server, toute la reconnaissance vocale, le LLM et la synth√®se vocale fonctionnent localement. Les options non locales (OpenAI ou autres API) sont facultatives.

**Q: How come it is older than manus ?**

Nous avons commenc√© cela comme un projet amusant pour cr√©er une IA locale de type Jarvis. Cependant, avec l‚Äô√©mergence de Manus, nous avons vu l‚Äôopportunit√© de r√©orienter certaines t√¢ches pour en faire une autre alternative.

**Q: How is it better than manus  ?**

Il ne l‚Äôest pas, mais nous privil√©gions l‚Äôex√©cution locale et la confidentialit√© par rapport √† une approche bas√©e sur le cloud. C‚Äôest une alternative amusante et accessible !

## Contribute

Nous recherchons des d√©veloppeurs pour am√©liorer AgenticSeek ! Consultez les probl√®mes ouverts ou les discussions.

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Auteurs:
 > [Fosowl](https://github.com/Fosowl)
 > [steveh8758](https://github.com/steveh8758) 
